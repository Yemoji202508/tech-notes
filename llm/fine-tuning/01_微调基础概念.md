# 大模型微调基础概念

## 什么是微调（Fine-tuning）

### 通俗理解

想象你雇了一个大学毕业生（预训练模型），他知识面很广，但不懂你公司的具体业务。微调就像是给他做**岗前培训**，让他学会你公司的专业术语、工作流程和业务规则。

**举个例子**：
- **预训练模型**：就像一个会说中文的人，能聊天、写文章、回答常识问题
- **微调后的模型**：变成了一个**医疗客服专家**，不仅会说中文，还精通医学术语、能准确回答病症问题、懂得医患沟通技巧

### 微调的过程

```
预训练模型（通用能力）
    ↓
准备专业数据集（你的业务数据）
    ↓
在小数据集上继续训练（微调）
    ↓
专业领域模型（垂直能力）
```

**具体步骤**：
1. **选择基座模型**：比如 LLaMA-3-8B（就像选择一个基础员工）
2. **准备训练数据**：整理你的专业问答对、文档（培训教材）
3. **配置训练参数**：学习率、训练轮数等（培训计划）
4. **开始训练**：让模型学习你的数据（上课培训）
5. **评估效果**：测试模型在专业任务上的表现（考试）

## 微调的优势

### 1. 成本低廉
- **从头训练**：需要数千万元的算力成本（培养一个博士）
- **微调**：只需几百到几千元（岗前培训）

### 2. 数据需求少
- **预训练**：需要 TB 级别的海量数据
- **微调**：几千到几万条专业数据就够了

### 3. 效果显著
在特定领域的表现可以超越通用大模型：
- 医疗诊断准确率提升 30%+
- 法律文书生成质量提升 50%+
- 客服响应准确率提升 40%+

### 4. 保护隐私
- 数据不用上传到第三方
- 可以在本地或私有云训练
- 适合处理敏感业务数据

### 5. 可控性强
- 可以让模型学习特定的回答风格
- 可以注入企业的价值观和规范
- 可以过滤不想要的输出

## 微调 vs 提示词工程

| 对比项 | 提示词工程 | 微调 |
|--------|-----------|------|
| 成本 | 几乎为零 | 需要算力和时间 |
| 效果 | 适合简单任务 | 适合复杂专业任务 |
| 数据要求 | 不需要数据 | 需要标注数据 |
| 响应速度 | 较慢（长提示词） | 较快（知识已内化） |
| 适用场景 | 快速验证、通用任务 | 垂直领域、高频任务 |

## 常见微调场景

1. **客服机器人**：学习公司产品知识和服务话术
2. **代码助手**：学习公司代码规范和内部框架
3. **文档生成**：学习特定格式的报告、合同模板
4. **内容审核**：学习企业的内容标准和价值观
5. **数据分析**：学习行业术语和分析方法

## 微调数据格式

### 1. 对话格式（Chat Format）

最常用的格式，适合问答、对话系统：

```json
{
  "conversations": [
    {
      "role": "user",
      "content": "什么是机器学习？"
    },
    {
      "role": "assistant",
      "content": "机器学习是人工智能的一个分支..."
    }
  ]
}
```

### 2. 指令格式（Instruction Format）

适合任务型微调：

```json
{
  "instruction": "将以下文本翻译成英文",
  "input": "今天天气很好",
  "output": "The weather is nice today"
}
```

### 3. 补全格式（Completion Format）

适合文本生成：

```json
{
  "prompt": "从前有座山，",
  "completion": "山里有座庙，庙里有个老和尚..."
}
```

### 数据质量要点

**✅ 好的训练数据**：
- 格式统一、标注准确
- 覆盖各种场景和边界情况
- 长度适中（不要太长或太短）
- 语言自然、符合实际使用

**❌ 差的训练数据**：
- 格式混乱、标注错误
- 场景单一、缺乏多样性
- 包含敏感信息或偏见
- 机器翻译或低质量生成的内容

### 数据量建议

| 任务类型 | 最少 | 推荐 | 理想 |
|---------|------|------|------|
| 简单分类 | 500 | 2,000 | 5,000+ |
| 问答系统 | 1,000 | 5,000 | 10,000+ |
| 对话系统 | 2,000 | 10,000 | 50,000+ |
| 代码生成 | 2,000 | 10,000 | 50,000+ |

**注意**：质量 > 数量，1000 条高质量数据胜过 10000 条低质量数据。

## 常见微调场景

1. **客服机器人**：学习公司产品知识和服务话术
2. **代码助手**：学习公司代码规范和内部框架
3. **文档生成**：学习特定格式的报告、合同模板
4. **内容审核**：学习企业的内容标准和价值观
5. **数据分析**：学习行业术语和分析方法

## 微调的局限性

1. **不能增加新知识**：微调主要是"激活"已有知识，不能让模型学会全新的事实
2. **可能过拟合**：数据太少或训练过度，模型会"死记硬背"
3. **需要持续更新**：业务变化时需要重新微调
4. **质量依赖数据**：垃圾数据进，垃圾模型出

## 微调的类型

### 1. 监督微调（SFT - Supervised Fine-Tuning）

**读音**：S-F-T

最常见的微调方式，使用标注好的输入-输出对训练模型。

```
输入：用户问题
输出：期望的回答
```

**适用场景**：
- 问答系统
- 文本生成
- 代码补全
- 翻译任务

### 2. 指令微调（Instruction Tuning）

SFT 的一种特殊形式，使用指令格式的数据训练。

```
指令：请将以下文本翻译成英文
输入：今天天气很好
输出：The weather is nice today
```

**特点**：
- 提升模型的指令遵循能力
- 增强零样本和少样本学习
- 提高任务泛化能力

### 3. 偏好对齐（Preference Alignment）

#### RLHF（Reinforcement Learning from Human Feedback）

**读音**：R-L-H-F

通过人类反馈的强化学习，让模型输出更符合人类偏好。

**流程**：
```
1. SFT 训练基础模型
2. 收集人类偏好数据（A 比 B 好）
3. 训练奖励模型
4. 用 PPO 算法优化策略
```

**代表模型**：ChatGPT、Claude

#### DPO（Direct Preference Optimization）

**读音**：D-P-O

直接优化偏好，不需要训练奖励模型，比 RLHF 更简单。

**优势**：
- 训练更稳定
- 实现更简单
- 效果接近 RLHF

**代表应用**：Llama 3、Mistral

### 4. 持续学习（Continual Learning）

在已微调的模型基础上继续学习新任务，同时保持旧任务的性能。

**挑战**：灾难性遗忘（学新忘旧）

**解决方案**：
- 经验回放（混入旧数据）
- 正则化方法
- 参数隔离（如 LoRA）

## 微调方法对比

| 类型 | 数据要求 | 训练难度 | 效果 | 适用场景 |
|------|---------|---------|------|---------|
| SFT | 输入-输出对 | 低 | 基础 | 特定任务 |
| 指令微调 | 指令格式数据 | 低 | 中 | 通用助手 |
| RLHF | 偏好标注 | 高 | 高 | 对齐人类价值观 |
| DPO | 偏好对比对 | 中 | 高 | 简化版 RLHF |

## 微调的局限性

1. **不能增加新知识**：微调主要是"激活"已有知识，不能让模型学会全新的事实
2. **可能过拟合**：数据太少或训练过度，模型会"死记硬背"
3. **需要持续更新**：业务变化时需要重新微调
4. **质量依赖数据**：垃圾数据进，垃圾模型出

## 下一步

- 了解 [LoRA 微调技术](./02_LoRA微调详解.md)
- 学习 [模型量化](./03_模型量化详解.md)
- 掌握 [模型评估方法](./04_模型评估方法.md)
