# 模型评估方法

## 为什么需要评估

微调完模型后，如何知道效果好不好？就像学生考试一样，模型也需要"考试"来检验学习成果。

### 评估的三个层次

```
1. 能不能用？    → 基础指标（困惑度、准确率）
2. 好不好用？    → 任务指标（BLEU、ROUGE）
3. 真实效果？    → 人工评估（A/B 测试）
```

## 评估方法分类

### 1. 自动评估（客观指标）

**优点**：
- ✅ 快速、可重复
- ✅ 成本低
- ✅ 便于对比

**缺点**：
- ❌ 不能完全反映真实效果
- ❌ 可能被"刷榜"

### 2. 人工评估（主观指标）

**优点**：
- ✅ 最接近真实使用场景
- ✅ 能评估创造性和流畅度

**缺点**：
- ❌ 成本高、耗时长
- ❌ 主观性强，不易重复

## 核心评估指标

### 1. 困惑度（Perplexity，PPL）

**读音**：困惑度（kun-huo-du）/ 帕普勒克西提（Perplexity）

#### 通俗理解

困惑度衡量模型对文本的"惊讶程度"：
- **低困惑度**：模型很"自信"，预测准确 → 好模型
- **高困惑度**：模型很"困惑"，预测不准 → 差模型

**举例**：
```
句子："今天天气很____"

好模型：
- "好" 概率 40%
- "热" 概率 30%
- "冷" 概率 20%
→ 困惑度低（选项集中）

差模型：
- "好" 概率 15%
- "热" 概率 12%
- "冷" 概率 10%
- ... 其他词概率分散
→ 困惑度高（不确定）
```

#### 计算公式

```
PPL = exp(平均交叉熵损失)
```

**数值参考**：
- PPL < 10：优秀
- PPL 10-20：良好
- PPL 20-50：一般
- PPL > 50：较差

#### 使用场景

- ✅ 语言模型预训练
- ✅ 对比不同模型
- ❌ 不适合生成任务（如翻译、摘要）

#### 代码示例

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained("your-model")
tokenizer = AutoTokenizer.from_pretrained("your-model")

text = "今天天气很好"
inputs = tokenizer(text, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs, labels=inputs["input_ids"])
    loss = outputs.loss
    ppl = torch.exp(loss)

print(f"困惑度: {ppl.item():.2f}")
```

### 2. 准确率（Accuracy）

#### 适用任务

- 分类任务（情感分析、意图识别）
- 选择题（多选一）

#### 计算公式

```
准确率 = 正确预测数 / 总样本数
```

#### 示例

```
测试集 100 条：
- 正确：85 条
- 错误：15 条
→ 准确率 = 85%
```

#### 局限性

**问题**：类别不平衡时会误导

```
数据集：
- 正样本：95 条
- 负样本：5 条

模型全预测为正：
- 准确率 = 95%（看起来很好）
- 但完全没学到负样本（实际很差）
```

**解决**：使用 F1 分数

### 3. F1 分数（F1 Score）

#### 相关概念

**精确率（Precision）**：预测为正的样本中，真正为正的比例
```
精确率 = 真正例 / (真正例 + 假正例)
```

**召回率（Recall）**：真正为正的样本中，被预测为正的比例
```
召回率 = 真正例 / (真正例 + 假负例)
```

**F1 分数**：精确率和召回率的调和平均
```
F1 = 2 × (精确率 × 召回率) / (精确率 + 召回率)
```

#### 通俗理解

**场景**：垃圾邮件检测

```
精确率：
- 你标记为垃圾邮件的，有多少真是垃圾？
- 高精确率 = 不会误杀正常邮件

召回率：
- 所有垃圾邮件中，你抓到了多少？
- 高召回率 = 不会漏掉垃圾邮件

F1 分数：
- 平衡两者，综合评价
```

#### 数值参考

- F1 > 0.9：优秀
- F1 0.7-0.9：良好
- F1 0.5-0.7：一般
- F1 < 0.5：较差

### 4. BLEU 分数

**全称**：Bilingual Evaluation Understudy
**读音**：布鲁（blue）

#### 适用任务

- 机器翻译
- 文本生成（有标准答案）

#### 原理

衡量生成文本和参考文本的 n-gram 重叠度

#### 示例

```
参考翻译：The cat is on the mat
模型翻译：The cat is sitting on the mat

1-gram 匹配：the(2), cat(1), is(1), on(1), the(1), mat(1) → 6/7
2-gram 匹配：the cat(1), cat is(1), on the(1), the mat(1) → 4/6
...

BLEU = 几何平均 × 长度惩罚
```

#### 数值参考

- BLEU > 40：优秀
- BLEU 30-40：良好
- BLEU 20-30：一般
- BLEU < 20：较差

#### 局限性

- 只看词汇重叠，不看语义
- 对同义词不友好
- 不适合创造性生成

### 5. ROUGE 分数

**全称**：Recall-Oriented Understudy for Gisting Evaluation
**读音**：如日（rouge）

#### 适用任务

- 文本摘要
- 文档生成

#### 类型

**ROUGE-N**：n-gram 召回率
```
ROUGE-1: 单词重叠
ROUGE-2: 双词重叠
```

**ROUGE-L**：最长公共子序列
```
考虑词序，更关注流畅度
```

#### 示例

```
参考摘要：今天天气很好，适合出门
生成摘要：今天天气不错，可以出门

ROUGE-1: 5/6 = 0.83（今天、天气、出门 等重叠）
ROUGE-2: 2/5 = 0.40（今天天气 重叠）
ROUGE-L: 4/6 = 0.67（最长公共子序列）
```

#### 数值参考

- ROUGE > 0.5：优秀
- ROUGE 0.4-0.5：良好
- ROUGE 0.3-0.4：一般
- ROUGE < 0.3：较差

### 6. 人工评估维度

#### 流畅度（Fluency）

- 1 分：完全不通顺，无法理解
- 3 分：基本通顺，有语法错误
- 5 分：完全流畅，无语法错误

#### 相关性（Relevance）

- 1 分：完全跑题
- 3 分：部分相关
- 5 分：完全相关

#### 准确性（Accuracy）

- 1 分：事实错误
- 3 分：部分正确
- 5 分：完全正确

#### 有用性（Helpfulness）

- 1 分：无用
- 3 分：有一定帮助
- 5 分：非常有用

## 标准评估基准

### 通用能力评估

#### 1. MMLU（Massive Multitask Language Understanding）

**读音**：M-M-L-U

- **内容**：57 个学科的选择题（数学、历史、法律等）
- **难度**：从小学到专业水平
- **用途**：评估模型的知识广度

**参考分数**：
- GPT-4: ~86%
- GPT-3.5: ~70%
- LLaMA-2-7B: ~45%

#### 2. GSM8K（Grade School Math 8K）

**读音**：G-S-M-8-K

- **内容**：8,500 道小学数学应用题
- **用途**：评估数学推理能力

**参考分数**：
- GPT-4: ~92%
- GPT-3.5: ~57%
- LLaMA-2-7B: ~14%

#### 3. HumanEval

- **内容**：164 道 Python 编程题
- **用途**：评估代码生成能力
- **指标**：Pass@1（一次通过率）

**参考分数**：
- GPT-4: ~67%
- GPT-3.5: ~48%
- CodeLlama-7B: ~30%

### 中文能力评估

#### 1. C-Eval

- **内容**：52 个学科的中文选择题
- **难度**：覆盖中学到专业水平
- **用途**：中文知识理解

#### 2. CMMLU（Chinese MMLU）

- **内容**：67 个主题的中文测试
- **特点**：更贴近中国教育体系

#### 3. AGIEval

- **内容**：中国高考、司法考试等真题
- **用途**：评估复杂推理能力

### 安全性评估

#### 1. TruthfulQA

- **用途**：评估模型是否会生成虚假信息
- **内容**：容易引发错误的问题

#### 2. ToxiGen

- **用途**：检测有害内容生成倾向
- **内容**：可能触发偏见的场景

## 评估流程

### 1. 准备测试集

#### 划分数据

```
总数据：10,000 条
├─ 训练集：8,000 条（80%）
├─ 验证集：1,000 条（10%）
└─ 测试集：1,000 条（10%）
```

**注意**：
- ⚠️ 测试集不能用于训练
- ⚠️ 测试集不能用于调参
- ⚠️ 测试集只用于最终评估

#### 数据质量

- ✅ 覆盖各种场景
- ✅ 难度分布合理
- ✅ 标注准确

### 2. 运行评估

#### 使用 LLaMA Factory

```bash
# 评估命令
llamafactory-cli eval \
  --model_name_or_path meta-llama/Llama-3-8B \
  --adapter_name_or_path ./output/lora \
  --eval_dataset test_data \
  --template llama3 \
  --output_dir ./eval_results
```

#### 自定义评估脚本

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm

# 加载模型
model = AutoModelForCausalLM.from_pretrained("your-model")
tokenizer = AutoTokenizer.from_pretrained("your-model")

# 加载测试集
test_data = load_dataset("your-test-set")

# 评估
correct = 0
total = 0

for example in tqdm(test_data):
    prompt = example["prompt"]
    ground_truth = example["answer"]
    
    # 生成
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=100)
    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # 判断
    if prediction.strip() == ground_truth.strip():
        correct += 1
    total += 1

accuracy = correct / total
print(f"准确率: {accuracy:.2%}")
```

### 3. 分析结果

#### 整体指标

```
模型：LLaMA-3-8B-LoRA
任务：客服问答

整体表现：
- 准确率：87.5%
- F1 分数：0.85
- 困惑度：12.3
```

#### 细分分析

```
按类别：
- 产品咨询：92%（好）
- 售后服务：85%（一般）
- 投诉处理：78%（需改进）

按难度：
- 简单问题：95%
- 中等问题：85%
- 复杂问题：72%
```

#### 错误分析

```
常见错误类型：
1. 事实错误（15%）
   - 产品参数记错
   - 政策理解偏差

2. 格式错误（10%）
   - 回答过长
   - 缺少关键信息

3. 语气问题（5%）
   - 过于生硬
   - 不够专业
```

## 实战建议

### 评估策略

#### 快速迭代期

```
主要指标：困惑度、准确率
评估频率：每个 epoch
目的：快速发现问题
```

#### 模型优化期

```
主要指标：F1、BLEU/ROUGE
评估频率：每次调参后
目的：精细调优
```

#### 上线前

```
主要指标：人工评估
评估规模：100-500 样本
目的：确保质量
```

### 常见问题

**Q1：指标很高但实际效果差？**

可能原因：
- 测试集泄露（和训练集重复）
- 测试集不代表真实场景
- 过拟合测试集

解决：
- 重新划分数据集
- 增加真实场景测试
- 人工抽查

**Q2：不同指标矛盾怎么办？**

```
场景：准确率高但 F1 低
原因：类别不平衡
解决：以 F1 为准，调整样本权重
```

**Q3：如何设定合格标准？**

```
参考基准：
1. 人类表现（上限）
2. 现有系统（对比）
3. 业务需求（底线）

示例：
- 人类客服准确率：95%
- 旧系统准确率：75%
- 业务要求：85%
→ 目标：85-90%
```

### 评估工具推荐

#### 1. LM Evaluation Harness

```bash
# 安装
pip install lm-eval

# 评估
lm_eval --model hf \
  --model_args pretrained=your-model \
  --tasks mmlu,hellaswag \
  --device cuda:0
```

**特点**：
- 支持 60+ 标准测试集
- 可重复、可对比
- 社区认可

#### 2. OpenCompass

```bash
# 中文模型评估
python run.py --models your-model --datasets ceval,cmmlu
```

**特点**：
- 专注中文评估
- 多维度对比
- 可视化报告

#### 3. 自定义评估

```python
# 使用 evaluate 库
from evaluate import load

# BLEU
bleu = load("bleu")
results = bleu.compute(predictions=preds, references=refs)

# ROUGE
rouge = load("rouge")
results = rouge.compute(predictions=preds, references=refs)
```

## 评估报告模板

```markdown
# 模型评估报告

## 基本信息
- 模型：LLaMA-3-8B-LoRA
- 任务：客服问答
- 训练数据：5,000 条
- 测试数据：500 条
- 评估日期：2025-11-15

## 自动评估结果

### 整体指标
| 指标 | 数值 | 基准 | 结论 |
|------|------|------|------|
| 准确率 | 87.5% | 85% | ✅ 达标 |
| F1 分数 | 0.85 | 0.80 | ✅ 达标 |
| 困惑度 | 12.3 | <15 | ✅ 达标 |

### 细分表现
- 产品咨询：92%
- 售后服务：85%
- 投诉处理：78%

## 人工评估结果（100 样本）

| 维度 | 平均分 | 说明 |
|------|--------|------|
| 流畅度 | 4.5/5 | 表达自然 |
| 相关性 | 4.2/5 | 回答切题 |
| 准确性 | 4.0/5 | 事实正确 |
| 有用性 | 4.3/5 | 解决问题 |

## 问题分析

### 主要问题
1. 复杂问题处理能力不足（72%）
2. 偶尔出现事实错误（15%）

### 改进建议
1. 增加复杂场景训练数据
2. 加强事实核查机制
3. 优化提示词模板

## 结论

模型整体达到上线标准，建议：
- ✅ 可用于简单和中等难度问题
- ⚠️ 复杂问题需人工介入
- 📅 计划 2 周后重新评估
```

## 总结

### 评估金字塔

```
         人工评估
        （最准确）
           ↑
      任务指标
   （BLEU/ROUGE）
        ↑
    基础指标
 （困惑度/准确率）
```

### 关键原则

1. **多维度评估**：不依赖单一指标
2. **真实场景**：测试集要代表实际使用
3. **持续监控**：上线后继续评估
4. **人机结合**：自动评估 + 人工抽查

### 快速检查清单

- [ ] 测试集独立，无泄露
- [ ] 覆盖各种场景和难度
- [ ] 至少 3 个评估指标
- [ ] 人工抽查 50+ 样本
- [ ] 对比基准模型
- [ ] 分析错误类型
- [ ] 记录评估报告

## 相关阅读

- [微调基础概念](./01_微调基础概念.md)
- [LoRA 微调详解](./02_LoRA微调详解.md)
- [模型量化详解](./03_模型量化详解.md)
- [LLaMA Factory 实战](./05_LLaMA_Factory实战.md)
