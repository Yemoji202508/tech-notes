# 模型量化详解

## 什么是量化（Quantization）

**读音**：量化（liang-hua）

### 通俗理解

想象你在拍照：
- **原始照片**（FP32）：每个像素用 32 位存储，文件 10MB，细节丰富
- **压缩照片**（INT8）：每个像素用 8 位存储，文件 2.5MB，肉眼看差不多
- **极限压缩**（INT4）：每个像素用 4 位存储，文件 1.25MB，稍有损失但可接受

量化就是**用更少的位数表示模型参数**，换取更小的体积和更快的速度。

### 为什么需要量化

**问题**：一个 70 亿参数的模型
- FP32（32 位浮点）：70 亿 × 4 字节 = 28GB
- 一张 RTX 4090 显卡：24GB 显存 → **装不下！**

**解决**：量化到 INT4
- 70 亿 × 0.5 字节 = 3.5GB → **轻松运行！**

## 精度类型详解

### 常见精度格式

| 格式 | 位数 | 字节 | 范围 | 精度 | 用途 |
|------|------|------|------|------|------|
| FP32 | 32 | 4 | ±3.4×10³⁸ | 最高 | 训练、科学计算 |
| FP16 | 16 | 2 | ±65504 | 高 | 训练、推理 |
| BF16 | 16 | 2 | ±3.4×10³⁸ | 中高 | 训练（更稳定） |
| INT8 | 8 | 1 | -128~127 | 中 | 推理 |
| INT4 | 4 | 0.5 | -8~7 | 低 | 推理（极限压缩） |

### 精度对比示例

用不同精度表示数字 `3.1415926`：

```
FP32:  3.14159265  （完整精度）
FP16:  3.14159     （略有损失）
BF16:  3.14160     （范围大但精度略低）
INT8:  3           （整数近似）
INT4:  3           （整数近似）
```

### FP16 vs BF16

**FP16**（Float16）：
- 1 位符号 + 5 位指数 + 10 位尾数
- 精度高，但容易溢出
- 适合推理

**BF16**（Brain Float16）：
- 1 位符号 + 8 位指数 + 7 位尾数
- 范围大（和 FP32 一样），不易溢出
- Google 为深度学习设计
- 适合训练

```
数值范围对比：
FP32:  ±3.4×10³⁸
BF16:  ±3.4×10³⁸  ← 和 FP32 一样
FP16:  ±65504     ← 容易溢出
```

## 量化方法分类

### 1. 按量化时机分类

#### PTQ（Post-Training Quantization，训练后量化）

**读音**：P-T-Q

**流程**：
```
训练完整模型（FP32）
    ↓
直接量化（INT8/INT4）
    ↓
量化模型
```

**优点**：
- ✅ 简单快速，几分钟搞定
- ✅ 不需要训练数据
- ✅ 不需要 GPU

**缺点**：
- ❌ 精度损失较大（5-10%）
- ❌ 极端量化（INT4）效果差

**适用场景**：
- 快速部署
- 对精度要求不高
- 没有训练资源

#### QAT（Quantization-Aware Training，量化感知训练）

**读音**：Q-A-T

**流程**：
```
训练时模拟量化
    ↓
模型学会适应量化误差
    ↓
量化模型（精度更高）
```

**优点**：
- ✅ 精度损失小（1-3%）
- ✅ 支持极端量化（INT4）

**缺点**：
- ❌ 需要重新训练
- ❌ 需要训练数据和 GPU
- ❌ 时间成本高

**适用场景**：
- 对精度要求高
- 有训练资源
- 生产环境部署

### 2. 按量化粒度分类

#### 逐层量化（Per-Tensor）

- 整个张量用同一个缩放因子
- 速度快，但精度略低

#### 逐通道量化（Per-Channel）

- 每个通道用不同的缩放因子
- 精度高，但计算稍慢

#### 分组量化（Group-wise）

- 将参数分组，每组用不同缩放因子
- GPTQ 和 AWQ 常用
- 平衡精度和速度

## 主流量化技术

### 1. GPTQ

**全称**：GPT-Quantization
**读音**：G-P-T-Q

#### 原理

- 基于**最优脑压缩**（Optimal Brain Compression）
- 逐层量化，最小化量化误差
- 使用少量校准数据（128-1024 样本）

#### 校准数据集的作用

**什么是校准数据**：
- 用于统计权重和激活值的分布
- 帮助确定最优的量化参数（缩放因子、零点）
- 不需要标签，只需要代表性文本

**常用校准数据集**：
```python
# C4 数据集（英文）
dataset="c4"

# WikiText（英文）
dataset="wikitext"

# 自定义数据（推荐）
dataset="your_domain_data"  # 使用你的业务数据效果更好
```

**数据量建议**：
- 最少：128 样本
- 推荐：512-1024 样本
- 更多不一定更好（边际收益递减）

#### 特点

```
优点：
✅ 精度高（接近原始模型）
✅ 支持 INT4/INT3 极限量化
✅ 一次量化，到处使用

缺点：
❌ 量化过程慢（几小时）
❌ 需要校准数据
❌ 推理速度一般
```

#### 使用场景

- 离线量化，长期使用
- 对精度要求高
- 显存极度受限（INT4）

#### 代码示例

```python
from transformers import AutoModelForCausalLM, GPTQConfig

# 配置 GPTQ
gptq_config = GPTQConfig(
    bits=4,                    # 量化到 4 位
    group_size=128,            # 分组大小
    dataset="c4",              # 校准数据集
    desc_act=False             # 是否量化激活值
)

# 加载并量化
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    quantization_config=gptq_config,
    device_map="auto"
)

# 保存量化模型
model.save_pretrained("./llama3-8b-gptq")
```

### 2. AWQ（Activation-aware Weight Quantization）

**读音**：A-W-Q

#### 原理

- 观察激活值分布
- 保护重要权重（激活值大的）
- 对不重要权重激进量化

#### 特点

```
优点：
✅ 推理速度快（比 GPTQ 快 1.5-2 倍）
✅ 精度高
✅ 量化速度快

缺点：
❌ 需要校准数据
❌ 显存占用略高于 GPTQ
```

#### 对比 GPTQ

| 对比项 | GPTQ | AWQ |
|--------|------|-----|
| 量化速度 | 慢（2-4 小时） | 快（30 分钟） |
| 推理速度 | 中等 | 快 |
| 精度 | 高 | 高 |
| 显存占用 | 低 | 中 |
| 适用场景 | 离线量化 | 在线服务 |

### 3. GGUF/GGML

**读音**：G-G-U-F / G-G-M-L

#### 特点

- llama.cpp 项目的量化格式
- 专为 CPU 推理优化
- 支持 Q2_K 到 Q8_0 多种量化级别

#### 量化级别

```
Q2_K: 2.5 位，极限压缩，精度损失大
Q3_K: 3.5 位，高压缩，精度尚可
Q4_K: 4.5 位，平衡选择（推荐）
Q5_K: 5.5 位，精度好
Q6_K: 6.5 位，接近原始
Q8_0: 8 位，高精度
```

#### 适用场景

- CPU 推理
- 边缘设备部署
- 本地运行（Ollama、LM Studio）

### 4. bitsandbytes（BNB）

**读音**：bits-and-bytes

#### 特点

- Hugging Face 官方支持
- 动态量化，加载时自动转换
- 支持 8-bit 和 4-bit

#### 使用示例

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# 4-bit 量化配置
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",      # 使用 NF4 量化
    bnb_4bit_compute_dtype="bfloat16",
    bnb_4bit_use_double_quant=True  # 双重量化
)

# 加载模型
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    quantization_config=bnb_config,
    device_map="auto"
)
```

#### NF4（NormalFloat4）

- 专为正态分布的权重设计
- 比标准 INT4 精度更高
- QLoRA 的核心技术

## 量化方法对比

### 综合对比表

| 方法 | 量化时间 | 推理速度 | 精度 | 显存占用 | 易用性 | 推荐场景 |
|------|---------|---------|------|---------|--------|---------|
| BNB 8-bit | 秒级 | 快 | 高 | 中 | ⭐⭐⭐⭐⭐ | 训练、推理 |
| BNB 4-bit | 秒级 | 快 | 中高 | 低 | ⭐⭐⭐⭐⭐ | QLoRA 训练 |
| GPTQ | 小时级 | 中 | 高 | 低 | ⭐⭐⭐ | 离线部署 |
| AWQ | 分钟级 | 快 | 高 | 中 | ⭐⭐⭐⭐ | 在线服务 |
| GGUF | 分钟级 | 中（CPU） | 中 | 低 | ⭐⭐⭐⭐ | CPU 推理 |

### 精度损失对比

以 LLaMA-2-7B 为例（相对 FP16 基准的相对性能保持率）：

```
FP16:        100%（基准）
BNB 8-bit:   ~99%（几乎无损）
AWQ 4-bit:   ~97%
GPTQ 4-bit:  ~96%
BNB 4-bit:   ~95%
GGUF Q4_K:   ~94%
GGUF Q3_K:   ~90%
GGUF Q2_K:   ~85%
```

**注意**：具体数值因模型、任务和评估指标而异，以上为典型参考值。

## 实战建议

### 场景选择指南

#### 1. 微调训练（QLoRA）

```python
# 推荐：BNB 4-bit
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype="bfloat16"
)
```

**原因**：
- 显存占用最低
- 支持梯度计算
- 精度损失可接受

#### 2. 在线推理服务

```bash
# 推荐：AWQ 4-bit
# 使用 vLLM 或 TGI 部署
```

**原因**：
- 推理速度快
- 精度高
- 吞吐量大

#### 3. 离线批量推理

```bash
# 推荐：GPTQ 4-bit
```

**原因**：
- 显存占用低
- 精度最高
- 速度要求不高

#### 4. 本地 CPU 运行

```bash
# 推荐：GGUF Q4_K
# 使用 Ollama 或 llama.cpp
```

**原因**：
- CPU 优化
- 易于部署
- 跨平台

### 常见问题

**Q1：量化会损失多少精度？**
- 8-bit：几乎无损（<1%）
- 4-bit：轻微损失（2-3%）
- 3-bit：明显损失（5-8%）
- 2-bit：严重损失（>10%）

**Q2：量化后能继续训练吗？**
- BNB 4-bit：✅ 可以（QLoRA）
- GPTQ/AWQ：❌ 不可以
- GGUF：❌ 不可以

**Q3：如何选择量化位数？**
```
显存充足     → FP16/BF16
显存紧张     → 8-bit
显存极限     → 4-bit
CPU 推理     → GGUF Q4_K
```

**Q4：量化后模型变小多少？**
```
FP32 → FP16:  50%（14GB → 7GB）
FP16 → INT8:  50%（7GB → 3.5GB）
FP16 → INT4:  75%（7GB → 1.75GB）
```

## 代码实战

### LLaMA Factory 中使用量化

```yaml
# 训练配置（QLoRA）
model_name_or_path: meta-llama/Llama-3-8B
quantization_bit: 4              # 4-bit 量化
quantization_method: bitsandbytes

# LoRA 配置
finetuning_type: lora
lora_rank: 8
```

### 量化已有模型

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# 加载原始模型
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3-8B")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3-8B")

# 量化配置
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    desc_act=False
)

# 执行量化
model = AutoGPTQForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B",
    quantize_config=quantize_config
)

# 保存
model.save_quantized("./llama3-8b-gptq")
```

## 总结

### 快速决策树

```
需要训练？
├─ 是 → BNB 4-bit（QLoRA）
└─ 否 → 需要推理
    ├─ GPU 推理
    │   ├─ 在线服务 → AWQ 4-bit
    │   └─ 离线批量 → GPTQ 4-bit
    └─ CPU 推理 → GGUF Q4_K
```

### 关键要点

1. **量化是压缩，不是魔法**：必然有精度损失
2. **4-bit 是甜点**：平衡精度和效率
3. **选对方法很重要**：训练用 BNB，推理看场景
4. **实测最重要**：不同模型和任务效果不同

## 相关阅读

- [LoRA 微调详解](./02_LoRA微调详解.md)
- [模型评估方法](./04_模型评估方法.md)
- [LLaMA Factory 实战](./05_LLaMA_Factory实战.md)
